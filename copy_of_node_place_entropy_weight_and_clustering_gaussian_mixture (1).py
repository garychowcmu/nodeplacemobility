# -*- coding: utf-8 -*-
"""Copy of Node place entropy weight and Clustering gaussian mixture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18mI1_9OPqBuIIWCMhVXDC5n-t0vUShoz
"""

#!/usr/bin/python
# -*- coding: utf-8 -*-

"""
Created on Fri Mar 23 10:48:36 2018
@author: Big Teacher Brother
"""
import pandas as pd
import numpy as np
import math
from numpy import array

df = pd.read_csv('hk node place mobility copy.csv')
df.dropna()
df
df = df[['POIs','Weighted_N','T0T__PP','TOT_WOR_PP']]
df

def cal_weight(x):
    x = x.apply(lambda x: ((x - np.min(x)) / (np.max(x) - np.min(x))))

    rows = x.index.size  # 行
    cols = x.columns.size  # 列
    k = 1.0 / math.log(rows)

    lnf = [[None] * cols for i in range(rows)]

    # p=array(p)
    x = array(x)
    lnf = [[None] * cols for i in range(rows)]
    lnf = array(lnf)
    for i in range(0, rows):
        for j in range(0, cols):
            if x[i][j] == 0:
                lnfij = 0.0
            else:
                p = x[i][j] / x.sum(axis=0)[j]
                lnfij = math.log(p) * p * (-k)
            lnf[i][j] = lnfij
    lnf = pd.DataFrame(lnf)
    E = lnf

    d = 1 - E.sum(axis=0)
    w = [[None] * 1 for i in range(cols)]
    for j in range(0, cols):
        wj = d[j] / sum(d)
        w[j] = wj

    w = pd.DataFrame(w)
    return w


if __name__ == '__main__':
    w = cal_weight(df)
    w.index = df.columns
    w.columns = ['weight']
    print(w)
    print('运行完成!')

def cal_weight(x):
    x = x.apply(lambda x: ((x - np.min(x)) / (np.max(x) - np.min(x))))

    rows = x.index.size  # 行
    cols = x.columns.size  # 列
    k = 1.0 / math.log(rows)

    lnf = [[None] * cols for i in range(rows)]


    x = array(x)
    lnf = [[None] * cols for i in range(rows)]
    lnf = array(lnf)
    for i in range(0, rows):
        for j in range(0, cols):
            if x[i][j] == 0:
                lnfij = 0.0
            else:
                p = x[i][j] / x.sum(axis=0)[j]
                lnfij = math.log(p) * p * (-k)
            lnf[i][j] = lnfij
    lnf = pd.DataFrame(lnf)
    E = lnf

    d = 1 - E.sum(axis=0)
    w = [[None] * 1 for i in range(cols)]
    for j in range(0, cols):
        wj = d[j] / sum(d)
        w[j] = wj

    w = pd.DataFrame(w)
    return w

df_score = df[['score_x','score_y']]
if __name__ == '__main__':
    #
    w = cal_weight(df_score)  #
    w.index = df_score.columns
    w.columns = ['weight']
    print(w)
    print('运行完成!')

import matplotlib.pyplot as plt
# from kneed import KneeLocator
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

# df = pd.read_csv('hk node place mobility copy.csv')
# df = pd.read_csv('mtr footprints.csv')
df = pd.read_csv('drive/MyDrive/final ppr node place copy 3.csv')


df

# x = df.loc[:, (df.columns != 'Cases')& (df.columns != 'Network Index')]
x = df.loc[:, (df.columns != 'weight')]

# select all columns except for travel time columns as x input for the model

x = x[['bus','poi','score_y','T0T__PP','TOT_WOR_PP','mix','score_x']]
# x = x[['bus_per','poi','score_y','T0T__PP','TOT_WOR_PP','mix','poi_rate1','pop_rate_poor']]

x

data_log = np.log(x)
data_log

# data_log['Place'] = data_log['poi']+data_log['Weighted_N']+data_log['T0T__PP']+data_log['TOT_WOR_PP']
data_log['Place'] = data_log['poi']+data_log['mix']+data_log['T0T__PP']+data_log['TOT_WOR_PP']

data_log['score_y'] = (data_log['score_y'] - data_log['score_y'].mean()) / data_log['score_y'].std()
data_log

data_log['score_x'] = (data_log['score_x'] - data_log['score_x'].mean()) / data_log['score_x'].std()
data_log

df['Network'] = 0.5*df['score_y']+0.5*df['score_x']

# data_log_sub = data_log[['Place','bus','score']]
# data_log_sub = data_log[['Place','bus']]
data_log_sub = data_log[['Place','bus','Network']]
# data_log_sub = data_log[['Place','bus']]

data_log_sub

import matplotlib.pyplot as plt
plt.hist(data_log_sub['Network'])

import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
min_max_scaler = preprocessing.MinMaxScaler()

data_log[['Place','bus','Network']]= min_max_scaler.fit_transform(data_log[['Place','bus','Network']])

X_train_minmax = min_max_scaler.fit_transform(data_log_sub)
X_train_minmax

data_log

data_log[['Place','bus','score']].to_csv('data log.csv')

data_log

import sklearn

from sklearn import mixture
kmeans = mixture.GaussianMixture(n_components=4, covariance_type='full')
# labels = gmm.predict(X)

kmeans = KMeans(
    init="random",
    n_clusters=4,
    random_state=0
)
# kmeans = KMeans(n_clusters=3, init='random', max_iter=300, n_init=10, random_state=0)